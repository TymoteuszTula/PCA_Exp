{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA_Exp tutorial\n",
    "\n",
    "This tutorial teaches how to use, and explains basic functionality of pca_exp package. The main task of pca_exp is to take a set of different experimental measurements, preprocess them, perform principal component analysis (PCA) and present the result in a user friendly format. Let's start from the quick explanation of PCA and why it is useful in analysis of experimental data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA \n",
    "\n",
    "Let's suppose that our experimental measurements look as follows:\n",
    "\n",
    "![title](figure/Figure_1.png)\n",
    "\n",
    "There is a subtle change of behaviour at some parameter value $T$, but we are not exactly sure where. Principal component analysis will allow us to find the most common deviations from the average of these curves (presented as a dashed line), called principal components (PC). By looking at the projections of experimental measurements onto different PCs, and how those projections differ with parameter $T$, we will be able to better understand the transition seen on figure above.\n",
    "\n",
    "First, we should import required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pca_exp.data_handler import DataHandler\n",
    "from pca_exp.pca_machine import PCAMachine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported two important classes: \"DataHandler\" loads, stores and preprocess the experimental measurement data, and \"PCAMachine\" is responsible for performing PCA and showing the results.\n",
    "\n",
    "The data on figure 1 should be in your working directory in the folder \"exp_data_example\". Each textfile in that folder correspond to different measurement and it has folowing format (in textfiles there are no headers):\n",
    "\n",
    "|X|Y|ErrorY|\n",
    "|---|---|---|\n",
    "|$x_1$|$y_1$|$ey_1$|\n",
    "|$x_2$|$y_2$|$ey_2$|\n",
    "|...|...|...|\n",
    "|$x_N$|$y_N$|$ey_N$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"DataHandler\" class can be used to load the textfiles if they are in similar format as the above. First, let's create instance of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DataHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class stores the data in its member \"batches\". To load all the data from \"exp_data_example\" folder we need to define few parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = './exp_data_example/'            # loc specifies the location of textfiles\n",
    "\n",
    "prenum = 'ede'                         # the class assumes that names of the textfiles in the folder \n",
    "stsp = (0, 10)                         # have form \"prenum(NUM)ext\" where NUM runs through integer numbers\n",
    "ext = '.txt'                           # from stsp[0] to stsp[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some additional options to consider loading the files (changing delimiter, skiping first rows etc.) but this is enough for this tutorial (for more options check data_handler.py). Loading the files require only one call: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.load_batch(stsp=stsp, loc=loc, prenum=prenum, ext=ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that we loaded files properly by displaying them directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(dh.batches[0][:,:,0], dh.batches[0][:,:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the curves above, we can see that the error seem to grow at later times. Useful tool in preprocessing such data is to re-bin the x-windows, so that each new bin captures approximately the same amount of standard deviation. This is done automatically by invoking \"prepare_XYE_PCA\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.prepare_XYE_PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function also prepares all the Y values in the matrix form such as:\n",
    "$$\n",
    " \\mathbf{Y} = \\begin{bmatrix}\n",
    "y_1(T_1) & y_1(T_2) & ... & y_1(T_M) \\\\\n",
    "y_2(T_1) & y_2(T_2) & ... & y_2(T_M) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{N'}(T_1) & y_{N'}(T_2) & ... & y_{N'}(T_M) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "which is necessery for PCA ($N' \\leq N$ because of re-binning)\n",
    "\n",
    "Let's create instance of \"PCAMachine\" that will perfom principal component analysis. We need to specify the DataHandler which holds the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_machine = PCAMachine(dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis consist of few steps:\n",
    "1. Substracting the average of $\\mathbf{Y}$ along the $T$ parameter. Namely we create new matrix $\\mathbf{Z}$, defined as\n",
    "$$\n",
    "[\\mathbf{Z}]_{ij} = y_i(T_j) - \\sum_k^M y_i(T_k).\n",
    "$$\n",
    "2. Performing singular value decomposition on $\\mathbf{Z}$. That is we find such matrices $\\mathbf{U}$, $\\mathbf{V}$ and diagonal matrix $\\mathbf{L}$ that\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{U} \\mathbf{L} \\mathbf{V}.\n",
    "$$\n",
    "The columns of $\\mathbf{U}$ matrix are principal components of $\\mathbf{Z}$. They are the same length as the columns of $\\mathbf{Y}$ and represent most common deviation from the average of all measurements. For each PC there is a singular value, which is written in diagonal $\\mathbf{L}$ matrix. It correspond to covariance captured by principal component. PCs with highest singular values are most important and from them we can learn correlation between measurement curves.\n",
    "3. Last important values are principal component scores, which are projections of measurement curves onto principal components:\n",
    "$$\n",
    "\\mathbf{Scores} = \\mathbf{U}^T \\mathbf{Z}.\n",
    "$$\n",
    "\n",
    "All of the above steps are performed by single call of \"PCAMachine\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_machine.perform_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results are stored as members of pca_machine: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pca_machine.pc_z[0]\n",
    "U = pca_machine.pc_curves[0]\n",
    "L = pca_machine.pc_sing[0]\n",
    "Scores = pca_machine.pc_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use pyplot to plot the results but there is also built-in option in pca_machine to do so, we should also specify the range of T parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [0.5, 0.6, 0.7, 0.8, 0.9, 1., 1.1, 1.2, 1.3, 1.4, 1.5]\n",
    "pca_machine.show_pca_results_1(param1=T, param1_name='T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the results. Looking at scree plots (which are singular values of principal components), we can see how many PCs are important to our analysis. In this case it seems that two first PCs are the most important ones.\n",
    "\n",
    "Perhaps the most relevant information lies in PC scores vs T parameter plots, from which we can learn at which T there is visible change in behaviour of experimental curves. Clearly, from two first PCs (which are the most important) we see that something must be happening between $T=0.9$ and $T=1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
